Notes about OSD install:

# add admin accounts

root@ceph05:~# apt-get install etckeeper


# install etckeeper

root@ceph05:~# apt-get install etckeeper



# add ssh key and distribute it in ceph cluster

root@ceph05:~# ssh-keygen

root@ceph01:~# ssh ceph05 cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys

## sync to all cluster nodes

root@ceph01:~# /home/dpavlin/ceph-tools/ceph-rsync /root/.ssh/authorized_keys 


# add node to ffzg.hr and net.ffzg.hr on deenes /etc/bind/hosts.db



# sync packages

root@ceph05:~# dpkg -l | grep ^ii | awk '{ print $2 }' > /tmp/dpkg.ceph05
root@ceph05:~# ssh ceph04 dpkg -l | grep ^ii | awk '{ print $2 }' > /tmp/dpkg.ceph04

root@ceph05:~# apt-get install ntp smartmontools megacli fail2ban

root@ceph05:~# tzconfig		# configure correct timezone

## add new node as ntp peer

root@ceph01:/etc# git diff
diff --git a/ntp.conf b/ntp.conf
index 5e4c636..f5b2533 100644
--- a/ntp.conf
+++ b/ntp.conf
@@ -30,6 +30,7 @@ peer ceph01
 peer ceph02
 peer ceph03
 peer ceph04
+peer ceph05

 # Access control configuration; see /usr/share/doc/ntp-doc/html/accopt.html for
 # details.  The web page <http://support.ntp.org/bin/view/Support/AccessRestrictions>

root@ceph01:~# /home/dpavlin/ceph-tools/ceph-rsync /etc/ntp.conf 

root@ceph01:~# /home/dpavlin/ceph-tools/ceph-ssh /etc/init.d/ntp restart

## verify that ntp works

root@ceph05:~# ntpq -p
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 10.80.3.255     .XFAC.          16 B    -   64    0    0.000    0.000   0.000
*zg1.ntp.CARNet. 161.53.123.8     2 u  715 1024  377    1.092    0.467   0.992
+zg2.ntp.CARNet. 161.53.123.8     2 u  477 1024  377    1.539    0.700   1.059
 ceph01.net.ffzg .STEP.          16 s    - 1024    0    0.000    0.000   0.000
 ceph02.net.ffzg .STEP.          16 s    - 1024    0    0.000    0.000   0.000
 ceph03.net.ffzg .STEP.          16 s    - 1024    0    0.000    0.000   0.000
 ceph04.net.ffzg .STEP.          16 s    - 1024    0    0.000    0.000   0.000
 ceph05.dhcp.ffz .STEP.          16 s    - 1024    0    0.000    0.000   0.000



# install munin-node, add ceph plugin, and add node to munin

root@ceph05:~# scp ceph01:/etc/munin/plugins/ceph_* /etc/munin/plugins/
ceph_capacity                                                                        100% 1519     1.5KB/s   00:00    
ceph_osd                                                                             100%  795     0.8KB/s   00:00    

root@ceph05:~# rsync -av ceph04:/etc/munin/smart_ /etc/munin/

root@ceph05:~# rsync -av 'ceph04:/etc/munin/plugins/smart_*' /etc/munin/plugins/

root@ceph05:~# rsync -av ceph04:/etc/munin/plugin-conf.d/ /etc/munin/plugin-conf.d/


root@ceph05:~# apt-get install bc # required by ceph_capacity

root@ceph05:~# /etc/init.d/munin-node restart
Restarting munin-node (via systemctl): munin-node.service.

# check network mtu on ceph network

root@ceph05:~# cat /etc/network/interfaces
auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet static
    address 10.80.3.48
    netmask 255.255.255.0
    mtu 9000

## verify that mtu works

root@ceph01:/home/dpavlin/ceph-tools# ping -c 1 -s 9000 ceph05
PING ceph05 (10.80.3.48) 9000(9028) bytes of data.
9008 bytes from ceph05 (10.80.3.48): icmp_seq=1 ttl=64 time=0.598 ms

--- ceph05 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.598/0.598/0.598/0.000 ms



# configure firewall

root@ceph05:~# apt-get install ferm

root@ceph05:/etc# git diff
diff --git a/ferm/ferm.conf b/ferm/ferm.conf
index fded11e..17f3f60 100644
--- a/ferm/ferm.conf
+++ b/ferm/ferm.conf
@@ -14,15 +14,16 @@ table filter {
         # allow local packet
         interface lo ACCEPT;
 
+        # allow vlan83 packet
+        interface eth1 ACCEPT;
+
         # respond to ping
         proto icmp ACCEPT; 
 
-        # allow IPsec
-        proto udp dport 500 ACCEPT;
-        proto (esp ah) ACCEPT;
-
         # allow SSH connections
         proto tcp dport ssh ACCEPT;
+
+        saddr 193.198.212.229 proto tcp dport 4949 ACCEPT;
     }
     chain OUTPUT {
         policy ACCEPT;

root@ceph05:/etc# /etc/init.d/ferm restart
Restarting ferm (via systemctl): ferm.service.


# syslog

root@ceph05:/etc# scp ceph01:/etc/rsyslog.d/ffzg.conf /etc/rsyslog.d/
ffzg.conf                                                                            100%  383     0.4KB/s   00:00    
root@ceph05:/etc# /etc/init.d/rsyslog restart
Restarting rsyslog (via systemctl): rsyslog.service.


# add osd to crush map to use it's space

root@r1u28:~# ceph osd tree
ID WEIGHT   TYPE NAME       UP/DOWN REWEIGHT PRIMARY-AFFINITY
-6  3.61578 host ceph05
 8  1.80789     osd.8            up  1.00000          1.00000
 9  1.80789     osd.9            up  1.00000          1.00000
-1 14.47998 root default
-2  3.62000     host ceph01
 0  1.81000         osd.0        up  1.00000          1.00000
 1  1.81000         osd.1        up  1.00000          1.00000
-3  3.62000     host ceph02
 2  1.81000         osd.2        up  1.00000          1.00000
 3  1.81000         osd.3        up  1.00000          1.00000
-4  3.62000     host ceph03
 4  1.81000         osd.4        up  1.00000          1.00000
 5  1.81000         osd.5        up  1.00000          1.00000
-5  3.62000     host ceph04
 6  1.81000         osd.6        up  1.00000          1.00000
 7  1.81000         osd.7        up  1.00000          1.00000

root@r1u28:~# ceph osd crush set osd.8 3.62 root=default
set item id 8 name 'osd.8' weight 3.62 at location {root=default} to crush map

root@r1u28:~# ceph osd tree
ID WEIGHT   TYPE NAME       UP/DOWN REWEIGHT PRIMARY-AFFINITY
-6  1.80789 host ceph05
 9  1.80789     osd.9            up  1.00000          1.00000
-1 18.09998 root default
-2  3.62000     host ceph01
 0  1.81000         osd.0        up  1.00000          1.00000
 1  1.81000         osd.1        up  1.00000          1.00000
-3  3.62000     host ceph02
 2  1.81000         osd.2        up  1.00000          1.00000
 3  1.81000         osd.3        up  1.00000          1.00000
-4  3.62000     host ceph03
 4  1.81000         osd.4        up  1.00000          1.00000
 5  1.81000         osd.5        up  1.00000          1.00000
-5  3.62000     host ceph04
 6  1.81000         osd.6        up  1.00000          1.00000
 7  1.81000         osd.7        up  1.00000          1.00000
 8  3.62000     osd.8            up  1.00000          1.00000

add second osd (root also seems to be ignored):

root@ceph01:~/ceph# ceph osd crush add 9 1.81 host=ceph05
set item id 9 name 'osd.9' weight 1.81 at location {host=ceph05}: no change

root@ceph01:~/ceph# ceph osd tree
ID WEIGHT   TYPE NAME       UP/DOWN REWEIGHT PRIMARY-AFFINITY 
-6  3.62000 host ceph05                                       
 8  1.81000     osd.8            up  1.00000          1.00000 
 9  1.81000     osd.9            up  1.00000          1.00000 
-1 14.47998 root default                                      
-2  3.62000     host ceph01                                   
 0  1.81000         osd.0        up  1.00000          1.00000 
 1  1.81000         osd.1        up  1.00000          1.00000 
-3  3.62000     host ceph02                                   
 2  1.81000         osd.2        up  1.00000          1.00000 
 3  1.81000         osd.3        up  1.00000          1.00000 
-4  3.62000     host ceph03                                   
 4  1.81000         osd.4        up  1.00000          1.00000 
 5  1.81000         osd.5        up  1.00000          1.00000 
-5  3.62000     host ceph04                                   
 6  1.81000         osd.6        up  1.00000          1.00000 
 7  1.81000         osd.7        up  1.00000          1.00000 



# now move ceph05 under root

root@ceph01:~/ceph# ceph osd crush move ceph05 root=default
moved item id -6 name 'ceph05' to location {root=default} in crush map

root@ceph01:~/ceph# ceph osd tree
ID WEIGHT   TYPE NAME       UP/DOWN REWEIGHT PRIMARY-AFFINITY 
-1 18.09998 root default                                      
-2  3.62000     host ceph01                                   
 0  1.81000         osd.0        up  1.00000          1.00000 
 1  1.81000         osd.1        up  1.00000          1.00000 
-3  3.62000     host ceph02                                   
 2  1.81000         osd.2        up  1.00000          1.00000 
 3  1.81000         osd.3        up  1.00000          1.00000 
-4  3.62000     host ceph03                                   
 4  1.81000         osd.4        up  1.00000          1.00000 
 5  1.81000         osd.5        up  1.00000          1.00000 
-5  3.62000     host ceph04                                   
 6  1.81000         osd.6        up  1.00000          1.00000 
 7  1.81000         osd.7        up  1.00000          1.00000 
-6  3.62000     host ceph05                                   
 8  1.81000         osd.8        up  1.00000          1.00000 
 9  1.81000         osd.9        up  1.00000          1.00000 


